{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "from pathlib import Path\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myahata\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.17"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/zamia/yahata/pair_programming/wandb/run-20220603_160210-3co911bz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/yahata/pair-programming/runs/3co911bz\" target=\"_blank\">wobbly-rain-3</a></strong> to <a href=\"https://wandb.ai/yahata/pair-programming\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/yahata/pair-programming/runs/3co911bz?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fa1fd0c0c40>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init(project=\"pair-programming\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG():\n",
    "    pretrained_model = '/larch/share/bert/NICT_pretrained_models/NICT_BERT-base_JapaneseWikipedia_32K_BPE'\n",
    "    pretrained_model_roberta = \"nlp-waseda/roberta-base-japanese\"\n",
    "    data_path = '/mnt/hinoki/karai/KUCI'\n",
    "    max_seq_len = 128\n",
    "    batch_size = 16\n",
    "    lr = 2e-5\n",
    "    weight_decay = 0.01\n",
    "    seed = 0\n",
    "    epoch = 3\n",
    "    warmup_ratio = 0.033\n",
    "    save_path = \"./result/bert\"\n",
    "    save_path_roberta = \"./result/roberta\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.log({\n",
    "    'max_seq_len': CFG.max_seq_len, \n",
    "    'batch_size': CFG.batch_size,\n",
    "    'lr': CFG.lr,\n",
    "    'weight_decay': CFG.weight_decay,\n",
    "    'seed': CFG.seed,\n",
    "    'epoch': CFG.epoch,\n",
    "    'warmup_ratio': CFG.epoch\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = Path(CFG.save_path)\n",
    "save_path.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = Path(CFG.save_path_roberta)\n",
    "save_path.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import re\n",
    "\n",
    "\n",
    "def set_seed(seed: int) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def set_device(gpuid: str) -> torch.device:\n",
    "    if gpuid and torch.cuda.is_available():\n",
    "        assert re.fullmatch(r\"[0-7]\", gpuid) is not None, \"invalid way to specify gpuid\"\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpuid\n",
    "        device = torch.device(f\"cuda:{gpuid}\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "set_seed(CFG.seed)\n",
    "device = set_device(\"0\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['今日', '##は', '##曇', '##り', '##です']\n",
      "{'input_ids': [[2, 8016, 26343, 29756, 26483, 26287, 3, 15108, 26343, 29746, 26507, 26292, 25974, 26302, 3], [2, 15108, 26343, 28149, 26894, 27927, 26287, 3, 274, 29556, 29573, 29822, 26149, 26222, 26302, 3]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    CFG.pretrained_model, do_lower_case=False, do_basic_tokenize=False\n",
    ")\n",
    "\n",
    "print(tokenizer.tokenize('今日は曇りです'))\n",
    "print(tokenizer([['今日は曇りです', '明日は晴れるといいな'],['明日はピクニックです', 'お弁当楽しみだな']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairPro(Dataset):\n",
    "    def __init__(self, path, tokenizer, max_seq_len, is_test=False):\n",
    "        self.is_test = is_test\n",
    "        self.label2int = {'a': 0, 'b': 1, 'c': 2, 'd': 3}\n",
    "        self.labels, self.contexts, self.choices = self.load(path)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_len = max_seq_len \n",
    "    \n",
    "    def load(self, path):\n",
    "        label_list = []\n",
    "        context_list = []\n",
    "        choice_list = []\n",
    "        with open(path) as f:\n",
    "            for i, line in enumerate(f):\n",
    "                problem = json.loads(line)\n",
    "                if self.is_test:\n",
    "                    label_list.append(-1)\n",
    "                else:\n",
    "                    label_list.append(self.label2int[problem['label']])\n",
    "                context_list.append(problem['context'])\n",
    "                choice_list.append([problem['choice_a'],problem['choice_b'],problem['choice_c'],problem['choice_d']])\n",
    "        assert len(label_list) == len(context_list), \"長さが違います\"\n",
    "        return label_list, context_list, choice_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input_list = []\n",
    "        for choice in self.choices[idx]:\n",
    "            input_list.append([self.contexts[idx], choice])    \n",
    "        return self.labels[idx], self.tokenizer(\n",
    "            input_list,\n",
    "            max_length=self.max_seq_len, \n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt',\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PairPro' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/mnt/zamia/yahata/pair_programming/pair-programming.ipynb Cell 11'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a226c6f7475732f626173696c343030227d/mnt/zamia/yahata/pair_programming/pair-programming.ipynb#ch0000010vscode-remote?line=0'>1</a>\u001b[0m traindataset \u001b[39m=\u001b[39m PairPro(CFG\u001b[39m.\u001b[39mdata_path\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m/train.jsonl\u001b[39m\u001b[39m'\u001b[39m, tokenizer, CFG\u001b[39m.\u001b[39mmax_seq_len)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a226c6f7475732f626173696c343030227d/mnt/zamia/yahata/pair_programming/pair-programming.ipynb#ch0000010vscode-remote?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(traindataset[\u001b[39m0\u001b[39m])  \u001b[39m# pairの要素を参照すると__getitem__が呼び出される\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a226c6f7475732f626173696c343030227d/mnt/zamia/yahata/pair_programming/pair-programming.ipynb#ch0000010vscode-remote?line=2'>3</a>\u001b[0m devdataset \u001b[39m=\u001b[39m PairPro(CFG\u001b[39m.\u001b[39mdata_path\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m/development.jsonl\u001b[39m\u001b[39m'\u001b[39m, tokenizer, CFG\u001b[39m.\u001b[39mmax_seq_len)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PairPro' is not defined"
     ]
    }
   ],
   "source": [
    "traindataset = PairPro(CFG.data_path+'/train.jsonl', tokenizer, CFG.max_seq_len)\n",
    "print(traindataset[0])  # pairの要素を参照すると__getitem__が呼び出される\n",
    "devdataset = PairPro(CFG.data_path+'/development.jsonl', tokenizer, CFG.max_seq_len)\n",
    "testdataset = PairPro(CFG.data_path+'/test.jsonl', tokenizer, CFG.max_seq_len, is_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindataloader = DataLoader(traindataset, batch_size=CFG.batch_size, shuffle=True, num_workers=2)\n",
    "devdataloader = DataLoader(devdataset, batch_size=CFG.batch_size, shuffle=True, num_workers=2)\n",
    "testdataloader = DataLoader(testdataset, batch_size=CFG.batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTPairPro(nn.Module):\n",
    "    def __init__(self, pretrained_model):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(\n",
    "            pretrained_model, output_attentions=False\n",
    "        )\n",
    "        self.linear = nn.Linear(\n",
    "            self.bert.config.hidden_size, 1\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        batch_size, n_choice, seq_len = input_ids.shape     # 16,4,128\n",
    "        input_ids = input_ids.view(batch_size*n_choice, seq_len)        # 64,128\n",
    "        attention_mask = attention_mask.view(batch_size*n_choice, seq_len)\n",
    "        token_type_ids = token_type_ids.view(batch_size*n_choice, seq_len)\n",
    "        # print(input_ids.shape)\n",
    "        output = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        cls = output['pooler_output']   # 64, 768\n",
    "        output = self.linear(cls)   # 64, 1\n",
    "        output = output.squeeze(1).view(batch_size, n_choice)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /larch/share/bert/NICT_pretrained_models/NICT_BERT-base_JapaneseWikipedia_32K_BPE were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BERTPairPro(CFG.pretrained_model)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type roberta to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n"
     ]
    }
   ],
   "source": [
    "model = BERTPairPro(CFG.pretrained_model_roberta)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, batch in enumerate(PairProdataloader):\n",
    "#     #print(batch)\n",
    "#     label, batch = batch\n",
    "#     if i == 0:\n",
    "#         output = model(batch['input_ids'], batch['attention_mask'], batch['token_type_ids'])\n",
    "#         print(output)\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=CFG.lr,\n",
    "    weight_decay=CFG.weight_decay,\n",
    "    no_deprecation_warning=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_training_steps = len(traindataloader) * CFG.epoch\n",
    "num_warmup_steps = num_training_steps * CFG.warmup_ratio\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_training_steps=num_training_steps,\n",
    "    num_warmup_steps=num_warmup_steps\n",
    ")\n",
    "\n",
    "ce_loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 33/5196 [00:15<39:51,  2.16it/s, lr=1.28e-6, loss=21.3] "
     ]
    }
   ],
   "source": [
    "best_score = None\n",
    "for epoch in range(CFG.epoch):\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    train_bar = tqdm(traindataloader)\n",
    "    for batch_idx, batch in enumerate(train_bar):\n",
    "        label, batch = batch \n",
    "        batch_size = len(batch['input_ids'])\n",
    "        label = label.to(device)\n",
    "        batch = {key: value.to(device) for key, value in batch.items()} \n",
    "        output = model(batch['input_ids'], batch['attention_mask'], batch['token_type_ids'])\n",
    "        loss = ce_loss(output, label)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item() * batch_size\n",
    "        train_bar.set_postfix(\n",
    "            {\n",
    "                'lr': scheduler.get_last_lr()[0],\n",
    "                'loss': round(total_loss / (batch_idx + 1), 3)\n",
    "            }\n",
    "        )\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        dev_bar = tqdm(devdataloader)\n",
    "        num_correct = 0\n",
    "        total_loss = 0\n",
    "        size = 0 \n",
    "        for batch_idx, batch in enumerate(dev_bar):\n",
    "            label, batch = batch \n",
    "            batch_size = len(batch['input_ids'])\n",
    "            label = label.to(device)\n",
    "            batch = {key: value.to(device) for key, value in batch.items()} \n",
    "            output = model(batch['input_ids'], batch['attention_mask'], batch['token_type_ids'])\n",
    "            loss = ce_loss(output, label)\n",
    "            predictions = torch.argmax(F.softmax(output, dim=1), dim=1)\n",
    "            num_correct += torch.sum(predictions == label).item()\n",
    "            size += batch_size\n",
    "            total_loss += loss.item() * batch_size\n",
    "            score = round(num_correct/size, 3)\n",
    "\n",
    "            dev_bar.set_postfix(\n",
    "               {\n",
    "                'size': size,\n",
    "                'accuracy': score,\n",
    "                'loss': round(total_loss / (batch_idx + 1), 3)\n",
    "                }\n",
    "            )\n",
    "            wandb.log(\n",
    "                {\n",
    "                    'step': size,\n",
    "                    'accuracy': score,\n",
    "                    'loss': total_loss / (batch_idx + 1)\n",
    "                }\n",
    "            )\n",
    "\n",
    "\n",
    "    score = round(num_correct/size, 3)\n",
    "    if best_score is None or score > best_score:\n",
    "        torch.save(model.state_dict(), CFG.save_path+\"/Checkpoint_best.pth\")\n",
    "        best_score = score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = torch.load(CFG.save_path+\"/Checkpoint_best.pth\", map_location=device)\n",
    "model.load_state_dict(state_dict)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = torch.load(CFG.save_path_roberta+\"/Checkpoint_best.pth\", map_location=device)\n",
    "model.load_state_dict(state_dict)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:05<00:00,  6.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['d', 'b', 'c', 'c', 'a', 'c', 'c', 'b', 'd', 'c', 'c', 'c', 'a', 'b', 'a', 'c', 'd', 'c', 'd', 'b', 'a', 'b', 'b', 'd', 'b', 'a', 'a', 'b', 'a', 'c', 'c', 'd', 'c', 'c', 'c', 'b', 'd', 'a', 'c', 'a', 'd', 'a', 'c', 'd', 'a', 'a', 'c', 'a', 'd', 'a', 'c', 'a', 'a', 'b', 'd', 'c', 'a', 'b', 'b', 'd', 'd', 'c', 'a', 'a', 'a', 'c', 'a', 'd', 'd', 'a', 'a', 'd', 'b', 'b', 'd', 'c', 'd', 'd', 'a', 'c', 'a', 'c', 'c', 'c', 'b', 'c', 'a', 'c', 'a', 'a', 'a', 'c', 'c', 'a', 'c', 'c', 'a', 'b', 'a', 'a', 'd', 'b', 'a', 'c', 'b', 'a', 'a', 'd', 'd', 'b', 'b', 'd', 'b', 'd', 'd', 'd', 'c', 'a', 'd', 'd', 'd', 'a', 'c', 'c', 'a', 'd', 'a', 'b', 'b', 'a', 'b', 'a', 'a', 'a', 'd', 'd', 'd', 'a', 'd', 'b', 'b', 'd', 'c', 'c', 'c', 'a', 'a', 'c', 'b', 'b', 'a', 'b', 'c', 'a', 'd', 'c', 'b', 'b', 'c', 'b', 'b', 'c', 'b', 'c', 'b', 'c', 'd', 'c', 'c', 'c', 'b', 'd', 'a', 'c', 'b', 'c', 'd', 'a', 'c', 'd', 'a', 'b', 'd', 'a', 'c', 'a', 'b', 'd', 'c', 'b', 'b', 'c', 'c', 'b', 'a', 'a', 'c', 'c', 'b', 'b', 'c', 'd', 'd', 'd', 'b', 'a', 'd', 'a', 'a', 'd', 'a', 'c', 'b', 'c', 'd', 'b', 'b', 'd', 'c', 'd', 'c', 'a', 'b', 'd', 'd', 'c', 'a', 'd', 'b', 'c', 'a', 'b', 'c', 'c', 'b', 'c', 'd', 'd', 'b', 'd', 'a', 'a', 'c', 'a', 'c', 'c', 'c', 'b', 'c', 'b', 'a', 'a', 'a', 'd', 'b', 'c', 'd', 'b', 'b', 'a', 'd', 'a', 'd', 'd', 'a', 'a', 'a', 'd', 'a', 'c', 'a', 'b', 'b', 'b', 'a', 'a', 'a', 'b', 'a', 'b', 'd', 'c', 'd', 'b', 'a', 'b', 'b', 'b', 'b', 'b', 'c', 'a', 'c', 'c', 'a', 'c', 'd', 'a', 'c', 'a', 'c', 'b', 'c', 'a', 'c', 'a', 'a', 'd', 'a', 'b', 'a', 'a', 'b', 'a', 'b', 'b', 'd', 'd', 'b', 'd', 'b', 'c', 'a', 'a', 'd', 'c', 'a', 'a', 'b', 'd', 'a', 'a', 'c', 'a', 'd', 'd', 'c', 'b', 'b', 'a', 'c', 'b', 'a', 'd', 'c', 'd', 'c', 'c', 'd', 'c', 'c', 'a', 'd', 'a', 'd', 'd', 'c', 'd', 'c', 'b', 'b', 'c', 'b', 'a', 'c', 'd', 'b', 'b', 'a', 'b', 'c', 'd', 'd', 'b', 'a', 'c', 'c', 'c', 'd', 'b', 'a', 'b', 'b', 'b', 'b', 'a', 'b', 'c', 'd', 'a', 'b', 'c', 'd', 'd', 'd', 'a', 'd', 'c', 'b', 'b', 'b', 'b', 'd', 'c', 'a', 'b', 'b', 'a', 'b', 'a', 'a', 'd', 'd', 'b', 'b', 'b', 'a', 'd', 'c', 'c', 'd', 'd', 'b', 'a', 'd', 'b', 'a', 'a', 'b', 'a', 'b', 'c', 'd', 'b', 'a', 'b', 'c', 'a', 'a', 'a', 'd', 'd', 'b', 'b', 'b', 'c', 'c', 'a', 'b', 'b', 'b', 'd', 'd', 'd', 'b', 'c', 'c', 'a', 'd', 'c', 'b', 'b', 'd', 'c', 'a', 'a', 'd', 'd', 'a', 'd', 'a', 'd', 'b', 'b', 'b', 'c', 'd', 'b', 'c', 'a', 'd', 'a', 'b', 'a', 'a', 'd', 'd', 'a', 'c', 'b', 'c', 'c', 'c', 'a', 'a', 'd', 'd', 'b', 'a', 'd']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "prediction = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_bar = tqdm(testdataloader)\n",
    "    num_correct = 0\n",
    "    total_loss = 0\n",
    "    size = 0 \n",
    "    for batch_idx, batch in enumerate(test_bar):\n",
    "        label, batch = batch \n",
    "        batch_size = len(batch['input_ids'])\n",
    "        # label = label.to(device)\n",
    "        batch = {key: value.to(device) for key, value in batch.items()} \n",
    "        output = model(batch['input_ids'], batch['attention_mask'], batch['token_type_ids'])\n",
    "        # loss = ce_loss(output, label)\n",
    "        predictions = torch.argmax(F.softmax(output, dim=1), dim=1)\n",
    "        size += batch_size\n",
    "        #total_loss += loss.item() * batch_size\n",
    "        prediction += predictions.tolist()\n",
    "    \n",
    "    int2label = {0: 'a', 1: 'b', 2: 'c', 3: 'd'}\n",
    "    prediction = [int2label[pre] for pre in prediction]\n",
    "    print(prediction)\n",
    "\n",
    "    #     #dev_bar.set_postfix(\n",
    "    #     {\n",
    "    #         'size': size,\n",
    "    #         #'accuracy': round(num_correct/size, 3),\n",
    "    #         #'loss': round(total_loss / (batch_idx + 1), 3)\n",
    "    #     }\n",
    "    # )\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_bar = tqdm(devdataloader)\n",
    "    num_correct = 0\n",
    "    total_loss = 0\n",
    "    size = 0 \n",
    "    for batch_idx, batch in enumerate(test_bar):\n",
    "        label, batch = batch \n",
    "        batch_size = len(batch['input_ids'])\n",
    "        # label = label.to(device)\n",
    "        batch = {key: value.to(device) for key, value in batch.items()} \n",
    "        output = model(batch['input_ids'], batch['attention_mask'], batch['token_type_ids'])\n",
    "        # loss = ce_loss(output, label)\n",
    "        predictions = torch.argmax(F.softmax(output, dim=1), dim=1)\n",
    "        size += batch_size\n",
    "        #total_loss += loss.item() * batch_size\n",
    "        prediction += predictions.tolist()\n",
    "    \n",
    "    int2label = {0: 'a', 1: 'b', 2: 'c', 3: 'd'}\n",
    "    prediction = [int2label[pre] for pre in prediction]\n",
    "    print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(CFG.save_path+\"/test_prediction.csv\", \"w\") as f:\n",
    "  csv.writer(f).writerows(prediction)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 宿題\n",
    "- 猿に勝つ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(path):\n",
    "    label_list = []\n",
    "    context_list = []\n",
    "    choice_list = []\n",
    "    agreement_list = []\n",
    "    with open(path) as f:\n",
    "        for i, line in enumerate(f):\n",
    "            problem = json.loads(line)\n",
    "            label_list.append(problem['label'])\n",
    "            context_list.append(problem['context'])\n",
    "            choice_list.append([problem['choice_a'],problem['choice_b'],problem['choice_c'],problem['choice_d']])\n",
    "            agreement_list.append(problem['agreement'])\n",
    "    assert len(label_list) == len(context_list), \"長さが違います\"\n",
    "    return label_list, context_list, choice_list, agreement_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_bert = []\n",
    "with open(CFG.save_path+'/dev_prediction.csv', 'r') as f:\n",
    "    predict = csv.reader(f)\n",
    "    for line in predict:\n",
    "            predictions_bert += line\n",
    "f.close()\n",
    "predictions_roberta = []\n",
    "with open(CFG.save_path_roberta+'/dev_prediction.csv', 'r') as f:\n",
    "    predict = csv.reader(f)\n",
    "    for line in predict:\n",
    "            predictions_roberta += line\n",
    "f.close()\n",
    "\n",
    "label, context, choice, agreement = load(CFG.data_path+'/development.jsonl')\n",
    "\n",
    "false = []\n",
    "for i, l in enumerate(zip(zip(agreement, label, predictions_bert, predictions_roberta), context, choice)):\n",
    "    if not l[0][3] == l[0][2]:\n",
    "        false.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((3, 'b', 'c', 'b'),\n",
      "  '夕方 から は 、 ちょっと だけ 雨 が 降った ので',\n",
      "  ['今朝 は ５ 時 前 に 目 が 覚め ます',\n",
      "   '結構 気温 が 下がり ます',\n",
      "   '日中 、 少し 雪 が とけた みたいだ',\n",
      "   '熱 が 出る']),\n",
      " ((3, 'c', 'a', 'd'),\n",
      "  '人 の 目 が なければ',\n",
      "  ['アメリカ みたいな 訳 に は 行か ない',\n",
      "   'やる 気 が 出 ない',\n",
      "   '道路 ルール を 守ら ない',\n",
      "   '巣 に 近づか ない ように して ください']),\n",
      " ((3, 'b', 'd', 'b'),\n",
      "  '他 の ＳＰＦ ５０ の 商品 と 比べる と',\n",
      "  ['安心 して ご 利用 に なれる', 'つけ 心地 は とっても 軽い', 'すっげー 気 が 楽だ', '使い 勝手 が 悪い']),\n",
      " ((2, 'a', 'c', 'a'),\n",
      "  'お 昼 は お出かけ した ので 、',\n",
      "  ['出先 の カフェ で パン を あげ ます',\n",
      "   '祝日 の 堂島 近辺 は 駅前 ビル まで 遠征 する',\n",
      "   '軽く 朝食 を 済ませる',\n",
      "   'なかなか 登山 電車 ウォッチング に 行け なく なって しまい ます']),\n",
      " ((3, 'a', 'c', 'a'),\n",
      "  '仕事 内容 が 変わる こと が あれば 、',\n",
      "  ['多分 もっと ネット 活用 する ように なる',\n",
      "   'お 車 できて も 便利だ',\n",
      "   'サービス 内容 など も 異なり ます',\n",
      "   '逆に 信用 を 失う']),\n",
      " ((2, 'd', 'a', 'd'),\n",
      "  'レベル も 上がった ので',\n",
      "  ['久々に ウォーキング に 出かけ ます',\n",
      "   'れいな の 指差す 先 に 視線 を 向ける',\n",
      "   '明日 は 幼稚園 に 行ける',\n",
      "   '玉座 方面 へ 向かう']),\n",
      " ((3, 'b', 'a', 'b'),\n",
      "  '遠い 家 で 雨戸 を 開ける 音 が 響く と 、',\n",
      "  ['なにやら 部屋 の 様子 が おかしい',\n",
      "   '私 は モヤモヤ した 頭 の まま 起き上がる',\n",
      "   '緊急 時 の 消火 器 の 使用 が 可能だ',\n",
      "   'クーラー の お 世話 に なり ます']),\n",
      " ((2, 'a', 'c', 'a'),\n",
      "  'やがて 雨 も 上がった ので',\n",
      "  ['昼食 を とり に 近所 の ラーメン 屋 さん に 行き ます',\n",
      "   '月曜 から は 学校 へ 行って いい そうだ',\n",
      "   '今日 は ミズホ スキー 場 に 行く',\n",
      "   'とうとう 米寿 前 に 昨日 入院 と なり ます']),\n",
      " ((4, 'a', 'd', 'a'),\n",
      "  '手 を つなぐ と',\n",
      "  ['心 の 距離 が 縮まり ます',\n",
      "   '増えた 体重 は すぐ 減り ます',\n",
      "   '帯域 等 の 関係 上 、 混線 の 可能 性 が ございます',\n",
      "   '心 が あらわれ ます']),\n",
      " ((2, 'b', 'c', 'b'),\n",
      "  '対応 は 遅い ので',\n",
      "  ['当日 の うち に 審査 結果 の 連絡 が あり ます',\n",
      "   '普通の 人 に は 我慢 が でき ない',\n",
      "   '慎重に 作業 を 進め ます',\n",
      "   '正式 版 は 後 で リリース する こと に なり ます']),\n",
      " ((2, 'c', 'd', 'a'),\n",
      "  '風 が 寒い ので 、',\n",
      "  ['服装 に は 準備 が 必要だ',\n",
      "   '色々な 種類 の カレー を 注文 し ます',\n",
      "   '午後 から は 、 ゆっくり 体 を 休める こと に し ます',\n",
      "   '自転車 に は 乗れ なく なる']),\n",
      " ((4, 'd', 'b', 'd'),\n",
      "  'ネット ショッピング を 利用 すれば 、',\n",
      "  ['楽しい 花火 も 火傷 や 事故 に つながり ます',\n",
      "   '広告 効果 が ぐんと 上がる',\n",
      "   'ヘルシンキ 観光 に は 便利だ',\n",
      "   '交通 費 が 浮き ます']),\n",
      " ((3, 'a', 'c', 'a'),\n",
      "  'うち の わんちゃん は あんまり 外 で 遊ば ない ので 、',\n",
      "  ['体重 が 心配だ', 'ちょっと 早 めに 出社 する', '会え ない の が 残念だ', '今日 は 疲れて 早く 寝 ます']),\n",
      " ((2, 'a', 'a', 'd'),\n",
      "  '夕方 から は 友人 と ご飯 を 食べる ので',\n",
      "  ['少し 気 が 楽だ',\n",
      "   '血糖 値 の コントロール が でき ない',\n",
      "   '一切 の 不安 無い',\n",
      "   '睡眠 不足 は いくらか 解消 した ようだ']),\n",
      " ((4, 'a', 'a', 'd'),\n",
      "  '耳 を 澄ます と',\n",
      "  ['途中 で 、 「 カシャンカシャン カシャン 」 と 機織 の 音 が 聞こえ ます',\n",
      "   '返事 する に も 大きい 声 出さ ない と いけない',\n",
      "   '声 良し 、 顔 良し で 耳 から 離れ ませ ん',\n",
      "   'この先 、 まだ 復旧 工事 を して いる のだろう 、 重機 の 重い エンジン の 音 が 聞こえる']),\n",
      " ((2, 'c', 'c', 'd'),\n",
      "  '会社 が 利益 を 出さ なければ',\n",
      "  ['収入 が 激減 し ます', '親 が きちんと 謝罪 も お礼 も し なきゃ', '借金 は 返せ ませ ん', '収入 は 増え ない']),\n",
      " ((2, 'd', 'd', 'c'),\n",
      "  '発生 した 責任 は 、 上司 に ある と 、',\n",
      "  ['一 つ は 、 曖昧な 返事 しか 出来 ない',\n",
      "   '県 外 の もの だった ため 盗ま れた もの か 確認 する',\n",
      "   '当然 検事 総長 に も 重い 責任 が ある',\n",
      "   '部下 は 上司 に ボール を 投げる']),\n",
      " ((3, 'a', 'a', 'c'),\n",
      "  '湿度 が 高い ため',\n",
      "  ['風通し の 良い 高床 式 で 壁 の ない 家 を 造る そうだ',\n",
      "   '当然 、 ゲーム 会社 に は お 金 が 一 銭 も 入ら ない わけだ',\n",
      "   '２ 時間 に １ 回 の 体位 交換 も 必要だ',\n",
      "   'これ は 、 何れ か 一方 の 溶着 ボス に おいて 、 過剰 溶融 又は 溶融 不足の 問題 が 発生 する']),\n",
      " ((2, 'a', 'c', 'a'),\n",
      "  '一 枚 の 写真 を 見せる と',\n",
      "  ['話 始める', 'ちょっと 考え も 変わる', '楽し さ 倍増 する', '状況 が 分から ない']),\n",
      " ((2, 'a', 'c', 'a'),\n",
      "  '鍵盤 わずか ４ ｍｍ でも 鍵盤 を 叩けば',\n",
      "  ['ちゃんと 音 を 出し ます',\n",
      "   '同じ ように 弾いて も しっかり した 音 が で ない',\n",
      "   'それなり の 音 が 出る ように なって き ます',\n",
      "   '傾けて も お 湯 が 出 ませ ん']),\n",
      " ((4, 'c', 'a', 'c'),\n",
      "  '今日 は 早 めに 家 に 着いた ので',\n",
      "  ['そっと 雨 の 音 を 聴く',\n",
      "   'ピアノ の 防音 工事 を 依頼 し ます',\n",
      "   'たっぷり ピアノ が 弾ける',\n",
      "   '音楽 に 合わせて 噴水 ショー が 始まる']),\n",
      " ((2, 'd', 'd', 'a'),\n",
      "  'コマ を まわす と',\n",
      "  ['目 が きょろきょろ 動く', '目 が キョロキョロ と 動き ます', '前輪 が 左右 に 動き ます', 'キョロちゃん が 起き上がる']),\n",
      " ((4, 'd', 'a', 'd'),\n",
      "  '最近 天気 が 不安定な ので',\n",
      "  ['家族 で おでかけ する', '体調 管理 たいへんだ', '開催 中止 する', '雷 に は 注意 し ます']),\n",
      " ((2, 'd', 'a', 'd'),\n",
      "  '朝 早く に 目 が 覚めた ので',\n",
      "  ['そのまま 映画 を 見る',\n",
      "   '午後 に 、 撮影 など 出来 そうだ',\n",
      "   '日曜日 に 催促 の 連絡 を する',\n",
      "   '久しぶりに スペイン リーグ の 放送 を 見る']),\n",
      " ((4, 'a', 'a', 'b'),\n",
      "  '無性に 喉 が 渇く ので',\n",
      "  ['ジュース や お 茶 ばかり 飲む', 'ラーメン を 食べる', '蕎麦 を 食う', '水分 のみ 摂る']),\n",
      " ((2, 'b', 'b', 'c'),\n",
      "  '時間 通り に こ れ ない 人 を 受け入れる のであれば 、',\n",
      "  ['ちょっと 緊張 する',\n",
      "   '自分 が 対処 できる ように 変わる',\n",
      "   '私 は 、 全面 的に 反対の 意向 を 表明 いたし ます',\n",
      "   '結構 大きい 声 が 出 ます']),\n",
      " ((4, 'c', 'b', 'c'),\n",
      "  '写真 も 撮ら ない と いけない ので',\n",
      "  ['ツイン ヒーロー ２ 体 分 の パーツ を 使用 する',\n",
      "   '会社 の 人 を 全員 呼び ませ ん でした',\n",
      "   '見える ところ に 移動 する',\n",
      "   '議論 の 流れ が 実に スムースだ']),\n",
      " ((4, 'c', 'a', 'c'),\n",
      "  '感動 したり 悲しんだり して 涙 を 流せば 、',\n",
      "  ['ストレス は たまる', '会社 に とって は 都合 が 悪い', 'スッキリ した 気分 に なれる', '中止 と なる']),\n",
      " ((3, 'c', 'a', 'c'),\n",
      "  '今月 契約 が 取れ なければ',\n",
      "  ['給料 を 貰え ない らしい', '安心 して 仕事 が でき ます', '職 を 失う らしい です', '世の中 の バランス は 崩れる']),\n",
      " ((3, 'b', 'd', 'b'),\n",
      "  '受注 も 単価 も 回復 の 見通し が 立た ない こと から 、',\n",
      "  ['かなり 高額な 治療 に なり ます',\n",
      "   '今回 の 事態 に 至る',\n",
      "   '声 が かかった 瞬間 、 驚き の 声 が 勝手に 漏れる',\n",
      "   '粗 利益 の 減少 が 予想 を 上回り ます']),\n",
      " ((3, 'c', 'b', 'c'),\n",
      "  '里山 の 風景 など 緩く のどかな 雰囲気 を 出す のであれば 、',\n",
      "  ['西桂 町 は 美味しい お 米 が とれ ます',\n",
      "   '近く に きっと 、 「 四万十 川 すみずみ ツーリズム 連絡 会 」 の 農家 民宿 や 体験 型 施設 、 農家 レストラン など が ある '\n",
      "   'はずだ',\n",
      "   'ある 程度 コントラスト や シャープネス の 低い 写真 の 方 が 味 に なり ます',\n",
      "   '通勤 の 時間 帯 は 多く の 人 で 混雑 し ます']),\n",
      " ((2, 'd', 'd', 'b'),\n",
      "  '常に 積極 的に 上がって シュート を 打った か と 思えば 、',\n",
      "  ['一般 の 人 は なかなか 投資 対象 に 選び ませ ん',\n",
      "   'ケンカ を 売ら ず に は い られ ない',\n",
      "   'チーム の 雰囲気 が 良い',\n",
      "   'チーム の ピンチ に は 必ず 顔 を 出す']),\n",
      " ((4, 'c', 'd', 'c'),\n",
      "  '状況 が 落ち着いた ような ので',\n",
      "  ['毎日 の 更新 は いたし ませ ん',\n",
      "   '詳細 は ソニー の サイト にて 要 確認 する',\n",
      "   '来週 の 週 末 から 映画 館 へ 映画 を 観 に 行く',\n",
      "   'デパート の 地下 へ 行く 事 に し ます']),\n",
      " ((3, 'b', 'a', 'b'),\n",
      "  'ポイント さえ 押さえれば 、',\n",
      "  ['誰 でも 登れる ように なり ます',\n",
      "   '「 売れる 文章 」 は 誰 でも 書け ます',\n",
      "   '初めて 滑った 時 が そのまま 大 事故 に なり ます',\n",
      "   '繊細な 操作 は あまり でき ませ ん']),\n",
      " ((2, 'b', 'd', 'b'),\n",
      "  '１００万 円 も 損して いる 事 を 知ったら 、',\n",
      "  ['手数 料 が １０００ 円 程 かかる らしい',\n",
      "   'コンタクト レンズ は もう 使え ませ ん',\n",
      "   '既存 の Ｗｅｂ アプリ に 対する 影響 が 大きい',\n",
      "   'ＰＳＰ 本体 が 買え ない']),\n",
      " ((3, 'd', 'a', 'd'),\n",
      "  'クジ を 引く と',\n",
      "  ['ボーナス 確定 する', '受け取り に 参り ます', '先生 の 場合 、 熱 が 上がり ます', '商品 が 当たり ます']),\n",
      " ((2, 'b', 'a', 'b'),\n",
      "  '保険 が 利か ない ので',\n",
      "  ['手続き は 面倒 かも しれ ませ ん',\n",
      "   '歯 医者 さん に よって 値段 は 違う',\n",
      "   '税 負担 の 公平 を 欠く こと に なって しまい ます',\n",
      "   'アドレス 、 電話 番号 は 変える']),\n",
      " ((2, 'b', 'd', 'b'),\n",
      "  '大手 の 物流 会社 に 比べれば 、',\n",
      "  ['期 中 に おいて 執行 役員 の 報酬 を 増額 し ます',\n",
      "   '名鉄 観光 サービス の 規模 は 小さい',\n",
      "   'フリー ＣＦ が マイナス と なる',\n",
      "   '価格 は 意外に 抑え めだ']),\n",
      " ((4, 'c', 'a', 'c'),\n",
      "  '歌って て 喉 が 渇いたら',\n",
      "  ['ビール 券 より も 切手 の ほう が ありがたい',\n",
      "   '寿司 屋 から 我が家 まで の 車 の 運転 は 奥さん に たのみ ます',\n",
      "   '電話 で ドリンク を 注文 し ます',\n",
      "   '宿泊 地 から 電車 で 由仁 へ と 移動 する']),\n",
      " ((2, 'b', 'b', 'a'),\n",
      "  '他 国 に 比べれば',\n",
      "  ['安い イメージ は ある',\n",
      "   'それ でも まだ 安全だ',\n",
      "   '必要な 訓練 の 規定 も ない',\n",
      "   'かなり コスト パフォーマンス が 向上 する']),\n",
      " ((2, 'a', 'b', 'd'),\n",
      "  '知識 不足の ため',\n",
      "  ['法的に 問題 が ある か まで は わかり ませ ん',\n",
      "   '合併 症 など の 予測 が 困難だ',\n",
      "   'やりがい を 感じ なく なって きて い ます',\n",
      "   '損失 を 被る 事 も ある']),\n",
      " ((2, 'b', 'a', 'b'),\n",
      "  '実際 に 友 チョコ を もらう と',\n",
      "  ['元気 出る', 'すごく テンション が あがる', '久々に カレー を 作る こと に し ます', 'すごく 抹茶 の 味 が 濃い']),\n",
      " ((3, 'd', 'c', 'd'),\n",
      "  '７ 月 に は お 休み が 取れた ので 、',\n",
      "  ['水曜日 は いったん 家 へ 帰り ます',\n",
      "   '私 は 、 大幅に 収入 も ダウン する',\n",
      "   '車 で きのこ 採り に いける ように なり ます',\n",
      "   '主人 と 一緒に 同封 して くださった 案内 の 佐原 大 祭り に 参加 する']),\n",
      " ((4, 'b', 'b', 'a'),\n",
      "  '卓球 の 選手 は 基本 的に 実業 団 選手 と なる ので 、',\n",
      "  ['より シビアな 戦い に なる だろう',\n",
      "   '所属 する 企業 の 正 社員 と して の 給料 が もらえる',\n",
      "   '下期 から 新しい 環境 整備 委員 会 が 発足 し ます',\n",
      "   '結果 発表 画面 に 移り ます']),\n",
      " ((3, 'c', 'd', 'b'),\n",
      "  '朝日 が 昇ったら 、',\n",
      "  ['さらに 多彩な 風景 が 広がり ます',\n",
      "   '鍋冠 山 公園 に 写真 を 撮り に 行き ます',\n",
      "   '自然 光 で 撮影 し ます',\n",
      "   '晴れた 日 に は 、 ３６０ 度 の 大 パノラマ は 広がる']),\n",
      " ((2, 'd', 'b', 'd'),\n",
      "  '考え 方 が まるで 違う から 、',\n",
      "  ['高 音質 でも 好み に 合わ ない こと も ある',\n",
      "   '人 に よって ずいぶん 違った イメージ が 湧く かも しれ ませ ん',\n",
      "   'あえて 言えば 機体 と の 相性 が 有る',\n",
      "   'さりげなく 傷つく ような 言葉 を 発する こと も ある']),\n",
      " ((3, 'a', 'b', 'a'),\n",
      "  'なんで 、 足 が 痛い 今年 の 夏 は 膝 を 痛めた ので 、',\n",
      "  ['立ち 仕事 なるべく 負担 を かけ ない ように して ます',\n",
      "   '筋肉 を 伸ばす ように し ます',\n",
      "   '多く の 観光 スポット が あり ます',\n",
      "   '別 世界 の ような 自然 豊かだ']),\n",
      " ((2, 'b', 'a', 'b'),\n",
      "  '２１ 日 まで に １ 日 余裕 が 生まれる から 、',\n",
      "  ['久しぶりに ブログ を 更新 し ます',\n",
      "   '出荷 数 の 調整 が 可能だ',\n",
      "   '確かに 電車 の 方 が 便利だ',\n",
      "   '対立 関係 に なる こと も ある ようだ']),\n",
      " ((3, 'd', 'a', 'd'),\n",
      "  '今 まで に 比べ 少し 量 が 多かった ので',\n",
      "  ['一 人 欠けた とき の リスク が 大きい',\n",
      "   '事前 に 時刻 を 確認 し ます',\n",
      "   '司法書士 で は なく 弁護 士 に 相談 し ます',\n",
      "   '驚いて 病院 へ 行き ます']),\n",
      " ((2, 'c', 'a', 'c'),\n",
      "  'トウモロコシ に 実 が ついた ので',\n",
      "  ['田んぼ の 水 を ギリギリ まで 抜く',\n",
      "   'ぴかぴか の 地 竹 が 顔 を 出し ます',\n",
      "   'カラス 避け の 網 を 張る',\n",
      "   'つまようじ で ゴミ を 取り除く'])]\n"
     ]
    }
   ],
   "source": [
    "import pprint as pp\n",
    "pp.pprint(false[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 'b', 'd', 'ＡＳＰ も 電子 納品 に 対応 した モノ が 多い です から 、', ['どんどん と 少子 化 は 進む だろう', '現場 の 負担 は 「 現状 より 」 は 確実に 減る', '希望 通り の サイト に 一 度 飛び ます', '無駄な ストレス を 感じ ませ ん']],\n",
      " [2, 'c', 'b', '歩いて いる 子ども 達 を 見る と 、', ['まったく チェック して い なかった 事 を 悔やむ', '教室 に は まだ チラホラ と しか 人 が い ない', '恐れ や 将来 に 対する 不安 など 無い ように 見える', '１ 本 の 背 の 高い 木 が 見える だろう']],\n",
      " [2, 'c', 'a', '風 が 寒い ので 、', ['服装 に は 準備 が 必要だ', '色々な 種類 の カレー を 注文 し ます', '午後 から は 、 ゆっくり 体 を 休める こと に し ます', '自転車 に は 乗れ なく なる']],\n",
      " [2, 'a', 'd', '夕方 から は 友人 と ご飯 を 食べる ので', ['少し 気 が 楽だ', '血糖 値 の コントロール が でき ない', '一切 の 不安 無い', '睡眠 不足 は いくらか 解消 した ようだ']],\n",
      " [2, 'c', 'd', '会社 が 利益 を 出さ なければ', ['収入 が 激減 し ます', '親 が きちんと 謝罪 も お礼 も し なきゃ', '借金 は 返せ ませ ん', '収入 は 増え ない']],\n",
      " [2, 'd', 'c', '発生 した 責任 は 、 上司 に ある と 、', ['一 つ は 、 曖昧な 返事 しか 出来 ない', '県 外 の もの だった ため 盗ま れた もの か 確認 する', '当然 検事 総長 に も 重い 責任 が ある', '部下 は 上司 に ボール を 投げる']],\n",
      " [2, 'd', 'a', 'コマ を まわす と', ['目 が きょろきょろ 動く', '目 が キョロキョロ と 動き ます', '前輪 が 左右 に 動き ます', 'キョロちゃん が 起き上がる']],\n",
      " [2, 'b', 'c', '時間 通り に こ れ ない 人 を 受け入れる のであれば 、', ['ちょっと 緊張 する', '自分 が 対処 できる ように 変わる', '私 は 、 全面 的に 反対の 意向 を 表明 いたし ます', '結構 大きい 声 が 出 ます']],\n",
      " [2, 'a', 'c', '見づらい 上 に 感度 が 高 すぎる ので', ['と いう か ぶっちゃけ 邪魔だ', 'うまく 付き合えば 戦力 に なる', '写真 は 載せ ない', 'お 月 様 が よく 見え ます']],\n",
      " [2, 'a', 'b', '近所 で 祭り が あった ので', ['ニコニコ し ながら カメラ 持参 する', '様子 を 見 に 行き ます', 'お 隣 ほのか ちゃん の 家 も ゴミ 出す', '若 おやぢ は 、 また 物凄い 勢い で 料理 を 頼み ます']],\n",
      " [2, 'd', 'b', '常に 積極 的に 上がって シュート を 打った か と 思えば 、', ['一般 の 人 は なかなか 投資 対象 に 選び ませ ん', 'ケンカ を 売ら ず に は い られ ない', 'チーム の 雰囲気 が 良い', 'チーム の ピンチ に は 必ず 顔 を 出す']],\n",
      " [2, 'c', 'b', '風 と 共に 舞う ように 剣 を 振るえば 、', ['靴 なんか 汚れる', '一瞬 の 後 、 使い 魔 の 首 が 吹き飛ぶ', '長く 伸びた 髪 が 揺れる', 'ぶん 殴り たい くらい 腹 が 立ち ます']],\n",
      " [2, 'a', 'd', 'アブラ メンツ が 全員 そろった って', ['こと が 嬉しい', '素敵な 特典 が ある そうだ', '食事 も 楽しい', '勢い が すごい']],\n",
      " [2, 'd', 'c', 'ｙｏｕｔｕｂｅ と か の ゲーム の プレイ 動画 見たら', ['ＰＣ に データ を ダウンロード し ます', 'エラー が 頻発 する', 'アレだ ね 、 面白い', '違い が よく わかる']],\n",
      " [2, 'd', 'b', 'ホテル に 戻る と', ['地下 道 から の アクセス も 便利だ', 'いよいよ 楽しい ハワイ 旅行 に 向けて の 搭乗 手続き に なり ます', 'この エリア は 、 ランチ に 最適だ', 'ホテル から の 景色 も とても 綺麗だ']],\n",
      " [2, 'b', 'a', '他 国 に 比べれば', ['安い イメージ は ある', 'それ でも まだ 安全だ', '必要な 訓練 の 規定 も ない', 'かなり コスト パフォーマンス が 向上 する']],\n",
      " [2, 'a', 'd', '知識 不足の ため', ['法的に 問題 が ある か まで は わかり ませ ん', '合併 症 など の 予測 が 困難だ', 'やりがい を 感じ なく なって きて い ます', '損失 を 被る 事 も ある']],\n",
      " [2, 'c', 'b', '嫁 も 子供 も 家 に 居 ない ので 、', ['野外 練習 は 中止 に し ます', '一 年間 お前 の 生活 を 援助 する', '今日 は 鍵 を 取り に 北 ビル に 行か なければ なら ない', '９ 時 頃 から 鍛接 炉 に 火 を つけ ます']],\n",
      " [2,\n",
      "  'a',\n",
      "  'c',\n",
      "  '東南 に ある 隣 の 家 の 壁 が 白い ため 、',\n",
      "  ['西日 の 太陽 光 が まぶしい 光 を 反射 する', 'ご 予定 や お腹 の 空き 具合 に 合わせて 、 都合 の 良い 時間 に ゆったり と お 食事 を 楽しむ', '真 新しい 木目 調 の 外観 が 目 を 引き ます', '始め なければ 島 の 姿 が 見え なく なって しまう ので は ない']],\n",
      " [2, 'a', 'd', '今朝 家 を 出る と', ['また 同じ ような 状況 、 たった １０ ｋｍ の 通勤 の 間 に ２ 件 も 追突 事故 を 見かける', 'アブ や 蚊 、 ハエ など 虫 たち は 全力 で 活動 する', 'お 昼 ご飯 も 札幌 駅 周辺 で いただき ます', '地元 の 朝市 で の もちつき 大会 に 参加 する']]]\n"
     ]
    }
   ],
   "source": [
    "false = []\n",
    "for i, predict in enumerate(predictions):\n",
    "    if not predict == label[i]:\n",
    "        f = [agreement[i], label[i], predict, context[i], choice[i]]\n",
    "        false.append(f)\n",
    "    \n",
    "agreement_2 = [line for line in false if line[0]==2]\n",
    "\n",
    "pp.pprint(agreement_2[:20], width=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({3: 3673, 2: 3587, 4: 2968})\n",
      "Counter({2: 568, 3: 441, 4: 292})\n",
      "{'2': 0.158, '3': 0.12, '4': 0.098}\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "cnt = collections.Counter(agreement)\n",
    "print(cnt)\n",
    "cnt_false = collections.Counter([line[0] for line in false])\n",
    "print(cnt_false)\n",
    "false_ratio = {'2':round(cnt_false[2]/cnt[2], 3), '3':round(cnt_false[3]/cnt[3], 3), '4':round(cnt_false[4]/cnt[4], 3)}\n",
    "print(false_ratio)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "81e02a2c5bfb011204f1239d0b22cac8959b9cca28fc4045ef66359d3a121ed4"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
