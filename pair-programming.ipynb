{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "from pathlib import Path\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG():\n",
    "    pretrained_model = '/larch/share/bert/NICT_pretrained_models/NICT_BERT-base_JapaneseWikipedia_32K_BPE'\n",
    "    path = '/mnt/hinoki/karai/KUCI'\n",
    "    max_seq_len = 128\n",
    "    batch_size = 16\n",
    "    lr = 2e-5\n",
    "    weight_decay = 0.01\n",
    "    seed = 0\n",
    "    epoch = 3\n",
    "    warmup_ratio = 0.033\n",
    "    save_path = \"./result/bert\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = Path(CFG.save_path)\n",
    "save_path.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import re\n",
    "\n",
    "\n",
    "def set_seed(seed: int) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def set_device(gpuid: str) -> torch.device:\n",
    "    if gpuid and torch.cuda.is_available():\n",
    "        assert re.fullmatch(r\"[0-7]\", gpuid) is not None, \"invalid way to specify gpuid\"\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpuid\n",
    "        device = torch.device(f\"cuda:{gpuid}\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "set_seed(CFG.seed)\n",
    "device = set_device(\"0\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['今日', '##は', '##曇', '##り', '##です']\n",
      "{'input_ids': [[2, 8016, 26343, 29756, 26483, 26287, 3, 15108, 26343, 29746, 26507, 26292, 25974, 26302, 3], [2, 15108, 26343, 28149, 26894, 27927, 26287, 3, 274, 29556, 29573, 29822, 26149, 26222, 26302, 3]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    CFG.pretrained_model, do_lower_case=False, do_basic_tokenize=False\n",
    ")\n",
    "\n",
    "print(tokenizer.tokenize('今日は曇りです'))\n",
    "print(tokenizer([['今日は曇りです', '明日は晴れるといいな'],['明日はピクニックです', 'お弁当楽しみだな']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairPro(Dataset):\n",
    "    def __init__(self, path, tokenizer, max_seq_len, is_test=False):\n",
    "        self.is_test = is_test\n",
    "        self.label2int = {'a': 0, 'b': 1, 'c': 2, 'd': 3}\n",
    "        self.labels, self.contexts, self.choices = self.load(path)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_len = max_seq_len \n",
    "    \n",
    "    def load(self, path):\n",
    "        label_list = []\n",
    "        context_list = []\n",
    "        choice_list = []\n",
    "        with open(path) as f:\n",
    "            for i, line in enumerate(f):\n",
    "                problem = json.loads(line)\n",
    "                if self.is_test:\n",
    "                    label_list.append(-1)\n",
    "                else:\n",
    "                    label_list.append(self.label2int[problem['label']])\n",
    "                context_list.append(problem['context'])\n",
    "                choice_list.append([problem['choice_a'],problem['choice_b'],problem['choice_c'],problem['choice_d']])\n",
    "        assert len(label_list) == len(context_list), \"長さが違います\"\n",
    "        return label_list, context_list, choice_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input_list = []\n",
    "        for choice in self.choices[idx]:\n",
    "            input_list.append([self.contexts[idx], choice])    \n",
    "        return self.labels[idx], self.tokenizer(\n",
    "            input_list,\n",
    "            max_length=self.max_seq_len, \n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt',\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, {'input_ids': tensor([[    2, 22845,  1135, 16968,  1159,  1484, 15060,  1143,    64,     3,\n",
      "         12091,  1135,  9871, 20394, 20353,  1159, 12081,   607,  1328,  1610,\n",
      "             3,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    2, 22845,  1135, 16968,  1159,  1484, 15060,  1143,    64,     3,\n",
      "          8010,  1159, 15449, 22617,   946,  1135,   274, 12503,  1110,  1094,\n",
      "          1324,     3,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    2, 22845,  1135, 16968,  1159,  1484, 15060,  1143,    64,     3,\n",
      "         10019, 17228,  1609, 13952,  8630,   701,     3,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    2, 22845,  1135, 16968,  1159,  1484, 15060,  1143,    64,     3,\n",
      "          8785, 17735,  1110, 25536,  3049, 21142,  1324,     3,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])})\n"
     ]
    }
   ],
   "source": [
    "traindataset = PairPro(CFG.path+'/train.jsonl', tokenizer, CFG.max_seq_len)\n",
    "print(traindataset[0])  # pairの要素を参照すると__getitem__が呼び出される\n",
    "devdataset = PairPro(CFG.path+'/development.jsonl', tokenizer, CFG.max_seq_len)\n",
    "testdataset = PairPro(CFG.path+'/test.jsonl', tokenizer, CFG.max_seq_len, is_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindataloader = DataLoader(traindataset, batch_size=CFG.batch_size, shuffle=True, num_workers=2)\n",
    "devataloader = DataLoader(devdataset, batch_size=CFG.batch_size, shuffle=True, num_workers=2)\n",
    "testdataloader = DataLoader(testdataset, batch_size=CFG.batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTPairPro(nn.Module):\n",
    "    def __init__(self, pretrained_model):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(\n",
    "            pretrained_model, output_attentions=False\n",
    "        )\n",
    "        self.linear = nn.Linear(\n",
    "            self.bert.config.hidden_size, 1\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        batch_size, n_choice, seq_len = input_ids.shape     # 16,4,128\n",
    "        input_ids = input_ids.view(batch_size*n_choice, seq_len)        # 64,128\n",
    "        attention_mask = attention_mask.view(batch_size*n_choice, seq_len)\n",
    "        token_type_ids = token_type_ids.view(batch_size*n_choice, seq_len)\n",
    "        # print(input_ids.shape)\n",
    "        output = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        cls = output['pooler_output']   # 64, 768\n",
    "        output = self.linear(cls)   # 64, 1\n",
    "        output = output.squeeze(1).view(batch_size, n_choice)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /larch/share/bert/NICT_pretrained_models/NICT_BERT-base_JapaneseWikipedia_32K_BPE were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = BERTPairPro(CFG.pretrained_model)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, batch in enumerate(PairProdataloader):\n",
    "#     #print(batch)\n",
    "#     label, batch = batch\n",
    "#     if i == 0:\n",
    "#         output = model(batch['input_ids'], batch['attention_mask'], batch['token_type_ids'])\n",
    "#         print(output)\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=CFG.lr,\n",
    "    weight_decay=CFG.weight_decay,\n",
    "    no_deprecation_warning=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_training_steps = len(traindataloader) * CFG.epoch\n",
    "num_warmup_steps = num_training_steps * CFG.warmup_ratio\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_training_steps=num_training_steps,\n",
    "    num_warmup_steps=num_warmup_steps\n",
    ")\n",
    "\n",
    "ce_loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 33/5196 [00:15<39:51,  2.16it/s, lr=1.28e-6, loss=21.3] "
     ]
    }
   ],
   "source": [
    "best_score = None\n",
    "for epoch in range(CFG.epoch):\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    train_bar = tqdm(traindataloader)\n",
    "    for batch_idx, batch in enumerate(train_bar):\n",
    "        label, batch = batch \n",
    "        batch_size = len(batch['input_ids'])\n",
    "        label = label.to(device)\n",
    "        batch = {key: value.to(device) for key, value in batch.items()} \n",
    "        output = model(batch['input_ids'], batch['attention_mask'], batch['token_type_ids'])\n",
    "        loss = ce_loss(output, label)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item() * batch_size\n",
    "        train_bar.set_postfix(\n",
    "            {\n",
    "                'lr': scheduler.get_last_lr()[0],\n",
    "                'loss': round(total_loss / (batch_idx + 1), 3)\n",
    "            }\n",
    "        )\n",
    "    \n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        dev_bar = tqdm(devataloader)\n",
    "        num_correct = 0\n",
    "        total_loss = 0\n",
    "        size = 0 \n",
    "        for batch_idx, batch in enumerate(dev_bar):\n",
    "            label, batch = batch \n",
    "            batch_size = len(batch['input_ids'])\n",
    "            label = label.to(device)\n",
    "            batch = {key: value.to(device) for key, value in batch.items()} \n",
    "            output = model(batch['input_ids'], batch['attention_mask'], batch['token_type_ids'])\n",
    "            loss = ce_loss(output, label)\n",
    "            predictions = torch.argmax(F.softmax(output, dim=1), dim=1)\n",
    "            num_correct += torch.sum(predictions == label).item()\n",
    "            size += batch_size\n",
    "            total_loss += loss.item() * batch_size\n",
    "            score = round(num_correct/size, 3)\n",
    "\n",
    "            dev_bar.set_postfix(\n",
    "            {\n",
    "                'size': size,\n",
    "                'accuracy': score,\n",
    "                'loss': round(total_loss / (batch_idx + 1), 3)\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    score = round(num_correct/size, 3)\n",
    "    if best_score is None or score > best_score:\n",
    "        torch.save(model.state_dict(), CFG.save_path+\"/Checkpoint_best.pth\")\n",
    "        best_score = score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /larch/share/bert/NICT_pretrained_models/NICT_BERT-base_JapaneseWikipedia_32K_BPE were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BERTPairPro(CFG.pretrained_model)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = torch.load(CFG.save_path+\"/Checkpoint_best.pth\", map_location=device)\n",
    "model.load_state_dict(state_dict)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:05<00:00,  6.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['d', 'b', 'c', 'c', 'a', 'c', 'c', 'b', 'd', 'c', 'c', 'c', 'a', 'b', 'a', 'c', 'd', 'c', 'd', 'b', 'a', 'b', 'b', 'd', 'b', 'a', 'a', 'b', 'a', 'c', 'c', 'd', 'c', 'c', 'c', 'b', 'd', 'a', 'c', 'a', 'd', 'a', 'c', 'd', 'a', 'a', 'c', 'a', 'd', 'a', 'c', 'a', 'a', 'b', 'd', 'c', 'a', 'b', 'b', 'd', 'd', 'c', 'a', 'a', 'a', 'c', 'a', 'd', 'd', 'a', 'a', 'd', 'b', 'b', 'd', 'c', 'd', 'd', 'a', 'c', 'a', 'c', 'c', 'c', 'b', 'c', 'a', 'c', 'a', 'a', 'a', 'c', 'c', 'a', 'c', 'c', 'a', 'b', 'a', 'a', 'd', 'b', 'a', 'c', 'b', 'a', 'a', 'd', 'd', 'b', 'b', 'd', 'b', 'd', 'd', 'd', 'c', 'a', 'd', 'd', 'd', 'a', 'c', 'c', 'a', 'd', 'a', 'b', 'b', 'a', 'b', 'a', 'a', 'a', 'd', 'd', 'd', 'a', 'd', 'b', 'b', 'd', 'c', 'c', 'c', 'a', 'a', 'c', 'b', 'b', 'a', 'b', 'c', 'a', 'd', 'c', 'b', 'b', 'c', 'b', 'b', 'c', 'b', 'c', 'b', 'c', 'd', 'c', 'c', 'c', 'b', 'd', 'a', 'c', 'b', 'c', 'd', 'a', 'c', 'd', 'a', 'b', 'd', 'a', 'c', 'a', 'b', 'd', 'c', 'b', 'b', 'c', 'c', 'b', 'a', 'a', 'c', 'c', 'b', 'b', 'c', 'd', 'd', 'd', 'b', 'a', 'd', 'a', 'a', 'd', 'a', 'c', 'b', 'c', 'd', 'b', 'b', 'd', 'c', 'd', 'c', 'a', 'b', 'd', 'd', 'c', 'a', 'd', 'b', 'c', 'a', 'b', 'c', 'c', 'b', 'c', 'd', 'd', 'b', 'd', 'a', 'a', 'c', 'a', 'c', 'c', 'c', 'b', 'c', 'b', 'a', 'a', 'a', 'd', 'b', 'c', 'd', 'b', 'b', 'a', 'd', 'a', 'd', 'd', 'a', 'a', 'a', 'd', 'a', 'c', 'a', 'b', 'b', 'b', 'a', 'a', 'a', 'b', 'a', 'b', 'd', 'c', 'd', 'b', 'a', 'b', 'b', 'b', 'b', 'b', 'c', 'a', 'c', 'c', 'a', 'c', 'd', 'a', 'c', 'a', 'c', 'b', 'c', 'a', 'c', 'a', 'a', 'd', 'a', 'b', 'a', 'a', 'b', 'a', 'b', 'b', 'd', 'd', 'b', 'd', 'b', 'c', 'a', 'a', 'd', 'c', 'a', 'a', 'b', 'd', 'a', 'a', 'c', 'a', 'd', 'd', 'c', 'b', 'b', 'a', 'c', 'b', 'a', 'd', 'c', 'd', 'c', 'c', 'd', 'c', 'c', 'a', 'd', 'a', 'd', 'd', 'c', 'd', 'c', 'b', 'b', 'c', 'b', 'a', 'c', 'd', 'b', 'b', 'a', 'b', 'c', 'd', 'd', 'b', 'a', 'c', 'c', 'c', 'd', 'b', 'a', 'b', 'b', 'b', 'b', 'a', 'b', 'c', 'd', 'a', 'b', 'c', 'd', 'd', 'd', 'a', 'd', 'c', 'b', 'b', 'b', 'b', 'd', 'c', 'a', 'b', 'b', 'a', 'b', 'a', 'a', 'd', 'd', 'b', 'b', 'b', 'a', 'd', 'c', 'c', 'd', 'd', 'b', 'a', 'd', 'b', 'a', 'a', 'b', 'a', 'b', 'c', 'd', 'b', 'a', 'b', 'c', 'a', 'a', 'a', 'd', 'd', 'b', 'b', 'b', 'c', 'c', 'a', 'b', 'b', 'b', 'd', 'd', 'd', 'b', 'c', 'c', 'a', 'd', 'c', 'b', 'b', 'd', 'c', 'a', 'a', 'd', 'd', 'a', 'd', 'a', 'd', 'b', 'b', 'b', 'c', 'd', 'b', 'c', 'a', 'd', 'a', 'b', 'a', 'a', 'd', 'd', 'a', 'c', 'b', 'c', 'c', 'c', 'a', 'a', 'd', 'd', 'b', 'a', 'd']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "prediction = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_bar = tqdm(testdataloader)\n",
    "    num_correct = 0\n",
    "    total_loss = 0\n",
    "    size = 0 \n",
    "    for batch_idx, batch in enumerate(test_bar):\n",
    "        label, batch = batch \n",
    "        batch_size = len(batch['input_ids'])\n",
    "        # label = label.to(device)\n",
    "        batch = {key: value.to(device) for key, value in batch.items()} \n",
    "        output = model(batch['input_ids'], batch['attention_mask'], batch['token_type_ids'])\n",
    "        # loss = ce_loss(output, label)\n",
    "        predictions = torch.argmax(F.softmax(output, dim=1), dim=1)\n",
    "        size += batch_size\n",
    "        #total_loss += loss.item() * batch_size\n",
    "        prediction += predictions.tolist()\n",
    "    \n",
    "    int2label = {0: 'a', 1: 'b', 2: 'c', 3: 'd'}\n",
    "    prediction = [int2label[pre] for pre in prediction]\n",
    "    print(prediction)\n",
    "\n",
    "    #     #dev_bar.set_postfix(\n",
    "    #     {\n",
    "    #         'size': size,\n",
    "    #         #'accuracy': round(num_correct/size, 3),\n",
    "    #         #'loss': round(total_loss / (batch_idx + 1), 3)\n",
    "    #     }\n",
    "    # )\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(CFG.save_path+\"/test_prediction.csv\", \"w\") as f:\n",
    "  csv.writer(f).writerows(prediction)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "81e02a2c5bfb011204f1239d0b22cac8959b9cca28fc4045ef66359d3a121ed4"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
